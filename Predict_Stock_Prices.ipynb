{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict_Stock_Prices.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeWRnu-Kh48Z",
        "outputId": "9b72ee16-2712-4629-cb7c-903776329d95"
      },
      "source": [
        "pip install yahoo_fin"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yahoo_fin\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/48/22c861ad5328b66aed12818901c2c2bd908f01a3de2b651d6f7ef05abd52/yahoo_fin-0.8.8-py3-none-any.whl\n",
            "Collecting requests-html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (1.1.5)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/58/0b/85d15e21f660a8ea68b1e0286168938857391f4ec9f6d204d91c9e013826/pyquery-1.4.3-py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/05/ea3250282e46fda60df1f1d5246bb8cdc022abb89969c61a98ea28fd6b82/pyppeteer-0.2.5-py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.7MB/s \n",
            "\u001b[?25hCollecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a1/82ce536be577ba09d4dcee45db58423a180873ad38a2d014d26ab7b7cb8a/parse-1.19.0.tar.gz\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from requests-html->yahoo_fin) (0.0.1)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (3.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (2018.9)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.7/dist-packages (from pyquery->requests-html->yahoo_fin) (4.2.6)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
            "Collecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.5MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<3.0.0,>=2.1.1; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/98/b8/8ec57a8ef46fbe7f185318c7ff7df9a06c9df451d9a59a067bfa851bb828/importlib_metadata-2.1.1-py2.py3-none-any.whl\n",
            "Collecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/0a/933b3931107e1da186963fd9bb9bceb9a613cff034cb0fb3b0c61003f357/pyee-8.1.0-py2.py3-none-any.whl\n",
            "Collecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from w3lib->requests-html->yahoo_fin) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->requests-html->yahoo_fin) (4.6.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<3.0.0,>=2.1.1; python_version < \"3.8\"->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.4.1)\n",
            "Building wheels for collected packages: fake-useragent, parse, sgmllib3k\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp37-none-any.whl size=13485 sha256=077770db59731c62859865da3586be6f90de7cee36317640a9b6eb03a83d4e41\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-cp37-none-any.whl size=24581 sha256=44046fcbe5e2e34ddc1dddf335ad290acebdd548054f92e800abccf6a944af7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/39/ea/e2fd678bd130953f5438470b8dfa529f00787e9b8b92b27467\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=5313e4454293b29b0b65a697bebbe1603a21649f8d48f3da8265f0c7cf967927\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built fake-useragent parse sgmllib3k\n",
            "\u001b[31mERROR: pyppeteer 0.2.5 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cssselect, pyquery, websockets, importlib-metadata, pyee, tqdm, pyppeteer, fake-useragent, parse, w3lib, requests-html, sgmllib3k, feedparser, yahoo-fin\n",
            "  Found existing installation: importlib-metadata 3.10.1\n",
            "    Uninstalling importlib-metadata-3.10.1:\n",
            "      Successfully uninstalled importlib-metadata-3.10.1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed cssselect-1.1.0 fake-useragent-0.1.11 feedparser-6.0.2 importlib-metadata-2.1.1 parse-1.19.0 pyee-8.1.0 pyppeteer-0.2.5 pyquery-1.4.3 requests-html-0.10.0 sgmllib3k-1.0.0 tqdm-4.60.0 w3lib-1.22.0 websockets-8.1 yahoo-fin-0.8.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLuHqrgKal-W"
      },
      "source": [
        "# packages\n",
        "\n",
        "from collections import deque\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from yahoo_fin import stock_info as si\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n77rW4ZUcU4R"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUI7k3hocfv2"
      },
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    # shuffle two arrays in the same way\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True, test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    \"\"\"\n",
        "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
        "    Params:\n",
        "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
        "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
        "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
        "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
        "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
        "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it to False will split datasets in a random way\n",
        "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
        "    \"\"\"\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instance\")\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    # add date as a column\n",
        "    if \"date\" not in df.columns:\n",
        "        df[\"date\"] = df.index\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
        "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    if split_by_date:\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - test_size) * len(X))\n",
        "        result[\"X_train\"] = X[:train_samples]\n",
        "        result[\"y_train\"] = y[:train_samples]\n",
        "        result[\"X_test\"]  = X[train_samples:]\n",
        "        result[\"y_test\"]  = y[train_samples:]\n",
        "        if shuffle:\n",
        "            # shuffle the datasets for training (if shuffle parameter is set)\n",
        "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
        "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
        "    else:    \n",
        "        # split the dataset randomly\n",
        "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                                test_size=test_size, shuffle=shuffle)\n",
        "    # get the list of test set dates\n",
        "    dates = result[\"X_test\"][:, -1, -1]\n",
        "    # retrieve test features from the original dataframe\n",
        "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
        "    # remove duplicated dates in the testing dataframe\n",
        "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
        "    # remove dates from the training/testing sets & convert to float32\n",
        "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    return result"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAcyAJDdczRt"
      },
      "source": [
        "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3, loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpQ02jUc6S6"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 50\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 15\n",
        "# whether to scale feature columns & output price as well\n",
        "SCALE = True\n",
        "scale_str = f\"sc-{int(SCALE)}\"\n",
        "# whether to shuffle the dataset\n",
        "SHUFFLE = True\n",
        "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
        "# whether to split the training/testing set by date\n",
        "SPLIT_BY_DATE = False\n",
        "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 2\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 200\n",
        "# Palantir stock market\n",
        "ticker = \"PLTR\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
        "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaYK5a66c93b"
      },
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0CnIuwuc_18",
        "outputId": "a7eeba6f-4ccd-4e9f-ec2e-ef17b747e3e3"
      },
      "source": [
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
        "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
        "                feature_columns=FEATURE_COLUMNS)\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "# train the model and save the weights whenever we see \n",
        "# a new optimal model using ModelCheckpoint\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 35s 2s/step - loss: 0.1532 - mean_absolute_error: 0.5296 - val_loss: 0.0434 - val_mean_absolute_error: 0.2714\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04338, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0743 - mean_absolute_error: 0.3397 - val_loss: 0.0315 - val_mean_absolute_error: 0.2039\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04338 to 0.03154, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0290 - mean_absolute_error: 0.1777 - val_loss: 0.0434 - val_mean_absolute_error: 0.2553\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03154\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0392 - mean_absolute_error: 0.2286 - val_loss: 0.0228 - val_mean_absolute_error: 0.1555\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03154 to 0.02282, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0227 - mean_absolute_error: 0.1503 - val_loss: 0.0108 - val_mean_absolute_error: 0.0943\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02282 to 0.01079, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0176 - mean_absolute_error: 0.1468 - val_loss: 0.0111 - val_mean_absolute_error: 0.1222\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01079\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0232 - mean_absolute_error: 0.1832 - val_loss: 0.0114 - val_mean_absolute_error: 0.0873\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01079\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0163 - mean_absolute_error: 0.1332 - val_loss: 0.0139 - val_mean_absolute_error: 0.0915\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01079\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0168 - mean_absolute_error: 0.1273 - val_loss: 0.0134 - val_mean_absolute_error: 0.0890\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01079\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0157 - mean_absolute_error: 0.1209 - val_loss: 0.0116 - val_mean_absolute_error: 0.0821\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01079\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0147 - mean_absolute_error: 0.1220 - val_loss: 0.0101 - val_mean_absolute_error: 0.0969\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01079 to 0.01006, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0175 - mean_absolute_error: 0.1495 - val_loss: 0.0099 - val_mean_absolute_error: 0.1071\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01006 to 0.00991, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0142 - mean_absolute_error: 0.1399 - val_loss: 0.0103 - val_mean_absolute_error: 0.0831\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00991\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0135 - mean_absolute_error: 0.1268 - val_loss: 0.0127 - val_mean_absolute_error: 0.0873\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00991\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0127 - mean_absolute_error: 0.1135 - val_loss: 0.0130 - val_mean_absolute_error: 0.0912\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00991\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0136 - mean_absolute_error: 0.1145 - val_loss: 0.0107 - val_mean_absolute_error: 0.0760\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00991\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0149 - mean_absolute_error: 0.1262 - val_loss: 0.0089 - val_mean_absolute_error: 0.0968\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00991 to 0.00889, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0184 - mean_absolute_error: 0.1535 - val_loss: 0.0086 - val_mean_absolute_error: 0.0935\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00889 to 0.00865, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0131 - mean_absolute_error: 0.1311 - val_loss: 0.0113 - val_mean_absolute_error: 0.0846\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00865\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0151 - mean_absolute_error: 0.1250 - val_loss: 0.0169 - val_mean_absolute_error: 0.1293\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00865\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0179 - mean_absolute_error: 0.1347 - val_loss: 0.0165 - val_mean_absolute_error: 0.1271\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00865\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0163 - mean_absolute_error: 0.1300 - val_loss: 0.0108 - val_mean_absolute_error: 0.0831\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00865\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0131 - mean_absolute_error: 0.1133 - val_loss: 0.0084 - val_mean_absolute_error: 0.0938\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00865 to 0.00843, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0146 - mean_absolute_error: 0.1396 - val_loss: 0.0085 - val_mean_absolute_error: 0.0922\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00843\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0118 - mean_absolute_error: 0.1195 - val_loss: 0.0093 - val_mean_absolute_error: 0.0757\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00843\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0109 - mean_absolute_error: 0.1094 - val_loss: 0.0099 - val_mean_absolute_error: 0.0763\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00843\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0134 - mean_absolute_error: 0.1143 - val_loss: 0.0091 - val_mean_absolute_error: 0.0781\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00843\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0098 - mean_absolute_error: 0.1017 - val_loss: 0.0085 - val_mean_absolute_error: 0.0934\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00843\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0130 - mean_absolute_error: 0.1349 - val_loss: 0.0087 - val_mean_absolute_error: 0.0831\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00843\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0119 - mean_absolute_error: 0.1213 - val_loss: 0.0110 - val_mean_absolute_error: 0.0864\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00843\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0140 - mean_absolute_error: 0.1177 - val_loss: 0.0129 - val_mean_absolute_error: 0.1032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00843\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0138 - mean_absolute_error: 0.1186 - val_loss: 0.0086 - val_mean_absolute_error: 0.0819\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00843\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0110 - mean_absolute_error: 0.1034 - val_loss: 0.0147 - val_mean_absolute_error: 0.1621\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00843\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0257 - mean_absolute_error: 0.1981 - val_loss: 0.0091 - val_mean_absolute_error: 0.1165\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00843\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0153 - mean_absolute_error: 0.1486 - val_loss: 0.0132 - val_mean_absolute_error: 0.1064\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00843\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0147 - mean_absolute_error: 0.1223 - val_loss: 0.0202 - val_mean_absolute_error: 0.1501\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00843\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0208 - mean_absolute_error: 0.1485 - val_loss: 0.0156 - val_mean_absolute_error: 0.1210\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00843\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0153 - mean_absolute_error: 0.1268 - val_loss: 0.0090 - val_mean_absolute_error: 0.0830\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00843\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0114 - mean_absolute_error: 0.1076 - val_loss: 0.0098 - val_mean_absolute_error: 0.1231\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00843\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0140 - mean_absolute_error: 0.1455 - val_loss: 0.0097 - val_mean_absolute_error: 0.1209\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00843\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0137 - mean_absolute_error: 0.1450 - val_loss: 0.0090 - val_mean_absolute_error: 0.0851\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00843\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0119 - mean_absolute_error: 0.1116 - val_loss: 0.0119 - val_mean_absolute_error: 0.0913\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00843\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0114 - mean_absolute_error: 0.0988 - val_loss: 0.0121 - val_mean_absolute_error: 0.0933\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00843\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0116 - mean_absolute_error: 0.1079 - val_loss: 0.0096 - val_mean_absolute_error: 0.0815\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00843\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0113 - mean_absolute_error: 0.1076 - val_loss: 0.0085 - val_mean_absolute_error: 0.0960\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00843\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0130 - mean_absolute_error: 0.1271 - val_loss: 0.0084 - val_mean_absolute_error: 0.0941\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00843 to 0.00841, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0091 - mean_absolute_error: 0.1043 - val_loss: 0.0096 - val_mean_absolute_error: 0.0836\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00841\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0109 - mean_absolute_error: 0.1077 - val_loss: 0.0126 - val_mean_absolute_error: 0.1057\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00841\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0134 - mean_absolute_error: 0.1156 - val_loss: 0.0125 - val_mean_absolute_error: 0.1060\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00841\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0134 - mean_absolute_error: 0.1163 - val_loss: 0.0099 - val_mean_absolute_error: 0.0875\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00841\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0104 - mean_absolute_error: 0.1054 - val_loss: 0.0081 - val_mean_absolute_error: 0.0885\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00841 to 0.00812, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0101 - mean_absolute_error: 0.1113 - val_loss: 0.0079 - val_mean_absolute_error: 0.0929\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00812 to 0.00792, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0108 - mean_absolute_error: 0.1133 - val_loss: 0.0080 - val_mean_absolute_error: 0.0878\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00792\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0095 - mean_absolute_error: 0.0985 - val_loss: 0.0101 - val_mean_absolute_error: 0.0953\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00792\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0090 - mean_absolute_error: 0.1018 - val_loss: 0.0129 - val_mean_absolute_error: 0.1161\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00792\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0135 - mean_absolute_error: 0.1266 - val_loss: 0.0108 - val_mean_absolute_error: 0.1030\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00792\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0106 - mean_absolute_error: 0.1075 - val_loss: 0.0077 - val_mean_absolute_error: 0.0942\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00792 to 0.00773, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0119 - mean_absolute_error: 0.1194 - val_loss: 0.0087 - val_mean_absolute_error: 0.1068\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00773\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0116 - mean_absolute_error: 0.1234 - val_loss: 0.0078 - val_mean_absolute_error: 0.0960\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00773\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0079 - mean_absolute_error: 0.0997 - val_loss: 0.0092 - val_mean_absolute_error: 0.0952\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00773\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0092 - mean_absolute_error: 0.1004 - val_loss: 0.0093 - val_mean_absolute_error: 0.0967\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00773\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0069 - mean_absolute_error: 0.0907 - val_loss: 0.0077 - val_mean_absolute_error: 0.0988\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00773 to 0.00772, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0081 - mean_absolute_error: 0.1030 - val_loss: 0.0112 - val_mean_absolute_error: 0.1233\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00772\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0096 - mean_absolute_error: 0.1076 - val_loss: 0.0079 - val_mean_absolute_error: 0.1031\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00772\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0060 - mean_absolute_error: 0.0816 - val_loss: 0.0062 - val_mean_absolute_error: 0.0986\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00772 to 0.00621, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0048 - mean_absolute_error: 0.0781 - val_loss: 0.0079 - val_mean_absolute_error: 0.0982\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00621\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0070 - mean_absolute_error: 0.0961 - val_loss: 0.0077 - val_mean_absolute_error: 0.0952\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00621\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0090 - mean_absolute_error: 0.1036 - val_loss: 0.0071 - val_mean_absolute_error: 0.0849\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00621\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0079 - mean_absolute_error: 0.0975 - val_loss: 0.0073 - val_mean_absolute_error: 0.0836\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00621\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0076 - mean_absolute_error: 0.0953 - val_loss: 0.0083 - val_mean_absolute_error: 0.0874\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00621\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0080 - mean_absolute_error: 0.0946 - val_loss: 0.0085 - val_mean_absolute_error: 0.0893\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00621\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0095 - mean_absolute_error: 0.0923 - val_loss: 0.0079 - val_mean_absolute_error: 0.0904\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00621\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0079 - mean_absolute_error: 0.0904 - val_loss: 0.0076 - val_mean_absolute_error: 0.0939\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00621\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0075 - mean_absolute_error: 0.0993 - val_loss: 0.0073 - val_mean_absolute_error: 0.0935\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00621\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0068 - mean_absolute_error: 0.0956 - val_loss: 0.0055 - val_mean_absolute_error: 0.0835\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.00621 to 0.00551, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0041 - mean_absolute_error: 0.0715 - val_loss: 0.0061 - val_mean_absolute_error: 0.0915\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00551\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0062 - mean_absolute_error: 0.0898 - val_loss: 0.0047 - val_mean_absolute_error: 0.0830\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.00551 to 0.00474, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0075 - mean_absolute_error: 0.0977 - val_loss: 0.0042 - val_mean_absolute_error: 0.0714\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.00474 to 0.00422, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0061 - mean_absolute_error: 0.0856 - val_loss: 0.0044 - val_mean_absolute_error: 0.0729\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00422\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0046 - mean_absolute_error: 0.0720 - val_loss: 0.0056 - val_mean_absolute_error: 0.0863\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00422\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0045 - mean_absolute_error: 0.0744 - val_loss: 0.0056 - val_mean_absolute_error: 0.0807\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00422\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0060 - mean_absolute_error: 0.0808 - val_loss: 0.0045 - val_mean_absolute_error: 0.0686\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00422\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0047 - mean_absolute_error: 0.0715 - val_loss: 0.0034 - val_mean_absolute_error: 0.0630\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.00422 to 0.00339, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0033 - mean_absolute_error: 0.0630 - val_loss: 0.0029 - val_mean_absolute_error: 0.0649\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.00339 to 0.00295, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0045 - mean_absolute_error: 0.0706 - val_loss: 0.0055 - val_mean_absolute_error: 0.0812\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00295\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0059 - mean_absolute_error: 0.0830 - val_loss: 0.0088 - val_mean_absolute_error: 0.1088\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00295\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0064 - mean_absolute_error: 0.0886 - val_loss: 0.0033 - val_mean_absolute_error: 0.0636\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00295\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0039 - mean_absolute_error: 0.0652 - val_loss: 0.0051 - val_mean_absolute_error: 0.0671\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00295\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0055 - mean_absolute_error: 0.0789 - val_loss: 0.0065 - val_mean_absolute_error: 0.0739\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00295\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0073 - mean_absolute_error: 0.0889 - val_loss: 0.0067 - val_mean_absolute_error: 0.0781\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00295\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0078 - mean_absolute_error: 0.0912 - val_loss: 0.0064 - val_mean_absolute_error: 0.0852\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00295\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0094 - mean_absolute_error: 0.1071 - val_loss: 0.0060 - val_mean_absolute_error: 0.0842\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00295\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0084 - mean_absolute_error: 0.1028 - val_loss: 0.0058 - val_mean_absolute_error: 0.0741\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00295\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0064 - mean_absolute_error: 0.0789 - val_loss: 0.0062 - val_mean_absolute_error: 0.0905\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00295\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0047 - mean_absolute_error: 0.0754 - val_loss: 0.0059 - val_mean_absolute_error: 0.0954\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00295\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0043 - mean_absolute_error: 0.0753 - val_loss: 0.0056 - val_mean_absolute_error: 0.0749\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00295\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0061 - mean_absolute_error: 0.0822 - val_loss: 0.0056 - val_mean_absolute_error: 0.0890\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00295\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0070 - mean_absolute_error: 0.0981 - val_loss: 0.0042 - val_mean_absolute_error: 0.0705\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00295\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0054 - mean_absolute_error: 0.0785 - val_loss: 0.0075 - val_mean_absolute_error: 0.0904\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00295\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0075 - mean_absolute_error: 0.0891 - val_loss: 0.0069 - val_mean_absolute_error: 0.0788\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00295\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0061 - mean_absolute_error: 0.0764 - val_loss: 0.0059 - val_mean_absolute_error: 0.0831\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00295\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0061 - mean_absolute_error: 0.0871 - val_loss: 0.0062 - val_mean_absolute_error: 0.0909\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00295\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0060 - mean_absolute_error: 0.0896 - val_loss: 0.0043 - val_mean_absolute_error: 0.0674\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00295\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0036 - mean_absolute_error: 0.0642 - val_loss: 0.0044 - val_mean_absolute_error: 0.0838\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00295\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0056 - mean_absolute_error: 0.0864 - val_loss: 0.0040 - val_mean_absolute_error: 0.0697\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.00295\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0050 - mean_absolute_error: 0.0741 - val_loss: 0.0054 - val_mean_absolute_error: 0.0889\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.00295\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0071 - mean_absolute_error: 0.0873 - val_loss: 0.0035 - val_mean_absolute_error: 0.0685\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.00295\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0041 - mean_absolute_error: 0.0698 - val_loss: 0.0066 - val_mean_absolute_error: 0.0857\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.00295\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0044 - mean_absolute_error: 0.0733 - val_loss: 0.0083 - val_mean_absolute_error: 0.1026\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.00295\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0049 - mean_absolute_error: 0.0773 - val_loss: 0.0076 - val_mean_absolute_error: 0.0955\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.00295\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0056 - mean_absolute_error: 0.0853 - val_loss: 0.0071 - val_mean_absolute_error: 0.0884\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.00295\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0064 - mean_absolute_error: 0.0863 - val_loss: 0.0060 - val_mean_absolute_error: 0.0802\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.00295\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0057 - mean_absolute_error: 0.0842 - val_loss: 0.0062 - val_mean_absolute_error: 0.0879\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.00295\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0065 - mean_absolute_error: 0.0963 - val_loss: 0.0054 - val_mean_absolute_error: 0.0867\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.00295\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0060 - mean_absolute_error: 0.0949 - val_loss: 0.0039 - val_mean_absolute_error: 0.0633\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.00295\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0045 - mean_absolute_error: 0.0723 - val_loss: 0.0047 - val_mean_absolute_error: 0.0850\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.00295\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0054 - mean_absolute_error: 0.0813 - val_loss: 0.0055 - val_mean_absolute_error: 0.0977\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.00295\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0052 - mean_absolute_error: 0.0845 - val_loss: 0.0056 - val_mean_absolute_error: 0.0931\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.00295\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0045 - mean_absolute_error: 0.0798 - val_loss: 0.0054 - val_mean_absolute_error: 0.0736\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.00295\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0060 - mean_absolute_error: 0.0826 - val_loss: 0.0067 - val_mean_absolute_error: 0.0863\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.00295\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0071 - mean_absolute_error: 0.0965 - val_loss: 0.0041 - val_mean_absolute_error: 0.0665\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.00295\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0039 - mean_absolute_error: 0.0658 - val_loss: 0.0046 - val_mean_absolute_error: 0.0815\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.00295\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0049 - mean_absolute_error: 0.0814 - val_loss: 0.0059 - val_mean_absolute_error: 0.0861\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00295\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0076 - mean_absolute_error: 0.0963 - val_loss: 0.0037 - val_mean_absolute_error: 0.0752\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00295\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0040 - mean_absolute_error: 0.0689 - val_loss: 0.0041 - val_mean_absolute_error: 0.0733\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.00295\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0040 - mean_absolute_error: 0.0650 - val_loss: 0.0057 - val_mean_absolute_error: 0.0829\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00295\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0039 - mean_absolute_error: 0.0692 - val_loss: 0.0068 - val_mean_absolute_error: 0.0873\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00295\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0056 - mean_absolute_error: 0.0822 - val_loss: 0.0071 - val_mean_absolute_error: 0.0921\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00295\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0061 - mean_absolute_error: 0.0825 - val_loss: 0.0073 - val_mean_absolute_error: 0.0915\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00295\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0061 - mean_absolute_error: 0.0842 - val_loss: 0.0071 - val_mean_absolute_error: 0.0896\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00295\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0063 - mean_absolute_error: 0.0808 - val_loss: 0.0059 - val_mean_absolute_error: 0.0822\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.00295\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0052 - mean_absolute_error: 0.0785 - val_loss: 0.0049 - val_mean_absolute_error: 0.0717\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00295\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0064 - mean_absolute_error: 0.0897 - val_loss: 0.0041 - val_mean_absolute_error: 0.0689\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00295\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0045 - mean_absolute_error: 0.0700 - val_loss: 0.0050 - val_mean_absolute_error: 0.0913\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.00295\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0050 - mean_absolute_error: 0.0818 - val_loss: 0.0076 - val_mean_absolute_error: 0.1143\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00295\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0062 - mean_absolute_error: 0.0975 - val_loss: 0.0048 - val_mean_absolute_error: 0.0883\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.00295\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0034 - mean_absolute_error: 0.0628 - val_loss: 0.0040 - val_mean_absolute_error: 0.0701\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00295\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0028 - mean_absolute_error: 0.0614 - val_loss: 0.0048 - val_mean_absolute_error: 0.0824\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.00295\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0043 - mean_absolute_error: 0.0738 - val_loss: 0.0050 - val_mean_absolute_error: 0.0751\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.00295\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0043 - mean_absolute_error: 0.0730 - val_loss: 0.0060 - val_mean_absolute_error: 0.0714\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00295\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0062 - mean_absolute_error: 0.0802 - val_loss: 0.0068 - val_mean_absolute_error: 0.0815\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00295\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0066 - mean_absolute_error: 0.0833 - val_loss: 0.0050 - val_mean_absolute_error: 0.0731\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00295\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0048 - mean_absolute_error: 0.0712 - val_loss: 0.0052 - val_mean_absolute_error: 0.0767\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00295\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0033 - mean_absolute_error: 0.0646 - val_loss: 0.0106 - val_mean_absolute_error: 0.1056\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00295\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0097 - mean_absolute_error: 0.1000 - val_loss: 0.0093 - val_mean_absolute_error: 0.0953\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00295\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0069 - mean_absolute_error: 0.0860 - val_loss: 0.0043 - val_mean_absolute_error: 0.0653\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00295\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0031 - mean_absolute_error: 0.0589 - val_loss: 0.0036 - val_mean_absolute_error: 0.0652\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.00295\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0040 - mean_absolute_error: 0.0663 - val_loss: 0.0047 - val_mean_absolute_error: 0.0732\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00295\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0045 - mean_absolute_error: 0.0746 - val_loss: 0.0044 - val_mean_absolute_error: 0.0688\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00295\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0039 - mean_absolute_error: 0.0678 - val_loss: 0.0031 - val_mean_absolute_error: 0.0616\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00295\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0034 - mean_absolute_error: 0.0655 - val_loss: 0.0032 - val_mean_absolute_error: 0.0616\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00295\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0032 - mean_absolute_error: 0.0619 - val_loss: 0.0045 - val_mean_absolute_error: 0.0669\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00295\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0035 - mean_absolute_error: 0.0636 - val_loss: 0.0052 - val_mean_absolute_error: 0.0717\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00295\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0045 - mean_absolute_error: 0.0702 - val_loss: 0.0058 - val_mean_absolute_error: 0.0733\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00295\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0049 - mean_absolute_error: 0.0766 - val_loss: 0.0069 - val_mean_absolute_error: 0.0813\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00295\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0060 - mean_absolute_error: 0.0812 - val_loss: 0.0073 - val_mean_absolute_error: 0.0871\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00295\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0070 - mean_absolute_error: 0.0919 - val_loss: 0.0062 - val_mean_absolute_error: 0.0825\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00295\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0052 - mean_absolute_error: 0.0762 - val_loss: 0.0039 - val_mean_absolute_error: 0.0667\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00295\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0036 - mean_absolute_error: 0.0626 - val_loss: 0.0036 - val_mean_absolute_error: 0.0641\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00295\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0034 - mean_absolute_error: 0.0623 - val_loss: 0.0055 - val_mean_absolute_error: 0.0743\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00295\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0068 - mean_absolute_error: 0.0858 - val_loss: 0.0056 - val_mean_absolute_error: 0.0900\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00295\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0045 - mean_absolute_error: 0.0732 - val_loss: 0.0046 - val_mean_absolute_error: 0.0721\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00295\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0043 - mean_absolute_error: 0.0705 - val_loss: 0.0058 - val_mean_absolute_error: 0.0703\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00295\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0059 - mean_absolute_error: 0.0783 - val_loss: 0.0063 - val_mean_absolute_error: 0.0731\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00295\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0066 - mean_absolute_error: 0.0836 - val_loss: 0.0064 - val_mean_absolute_error: 0.0794\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00295\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0078 - mean_absolute_error: 0.0932 - val_loss: 0.0063 - val_mean_absolute_error: 0.0844\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00295\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0058 - mean_absolute_error: 0.0839 - val_loss: 0.0059 - val_mean_absolute_error: 0.0831\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00295\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0066 - mean_absolute_error: 0.0928 - val_loss: 0.0048 - val_mean_absolute_error: 0.0674\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00295\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0067 - mean_absolute_error: 0.0860 - val_loss: 0.0050 - val_mean_absolute_error: 0.0775\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00295\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0054 - mean_absolute_error: 0.0768 - val_loss: 0.0064 - val_mean_absolute_error: 0.0952\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00295\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0064 - mean_absolute_error: 0.0959 - val_loss: 0.0051 - val_mean_absolute_error: 0.0877\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00295\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0047 - mean_absolute_error: 0.0770 - val_loss: 0.0028 - val_mean_absolute_error: 0.0587\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.00295 to 0.00280, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0027 - mean_absolute_error: 0.0575 - val_loss: 0.0043 - val_mean_absolute_error: 0.0792\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00280\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0053 - mean_absolute_error: 0.0857 - val_loss: 0.0046 - val_mean_absolute_error: 0.0753\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00280\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0041 - mean_absolute_error: 0.0704 - val_loss: 0.0045 - val_mean_absolute_error: 0.0720\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00280\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0031 - mean_absolute_error: 0.0598 - val_loss: 0.0053 - val_mean_absolute_error: 0.0831\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00280\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0047 - mean_absolute_error: 0.0797 - val_loss: 0.0049 - val_mean_absolute_error: 0.0775\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00280\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0046 - mean_absolute_error: 0.0729 - val_loss: 0.0039 - val_mean_absolute_error: 0.0652\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00280\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0035 - mean_absolute_error: 0.0638 - val_loss: 0.0032 - val_mean_absolute_error: 0.0632\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00280\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0037 - mean_absolute_error: 0.0651 - val_loss: 0.0030 - val_mean_absolute_error: 0.0626\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00280\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0039 - mean_absolute_error: 0.0637 - val_loss: 0.0045 - val_mean_absolute_error: 0.0763\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00280\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0051 - mean_absolute_error: 0.0766 - val_loss: 0.0070 - val_mean_absolute_error: 0.1008\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00280\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0062 - mean_absolute_error: 0.0870 - val_loss: 0.0045 - val_mean_absolute_error: 0.0730\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00280\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0031 - mean_absolute_error: 0.0612 - val_loss: 0.0040 - val_mean_absolute_error: 0.0649\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00280\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0040 - mean_absolute_error: 0.0701 - val_loss: 0.0033 - val_mean_absolute_error: 0.0608\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00280\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0024 - mean_absolute_error: 0.0566 - val_loss: 0.0030 - val_mean_absolute_error: 0.0682\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00280\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0023 - mean_absolute_error: 0.0529 - val_loss: 0.0030 - val_mean_absolute_error: 0.0617\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00280\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0039 - mean_absolute_error: 0.0709 - val_loss: 0.0030 - val_mean_absolute_error: 0.0574\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00280\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0054 - mean_absolute_error: 0.0807 - val_loss: 0.0030 - val_mean_absolute_error: 0.0591\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00280\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0023 - mean_absolute_error: 0.0527 - val_loss: 0.0035 - val_mean_absolute_error: 0.0672\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00280\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0042 - mean_absolute_error: 0.0685 - val_loss: 0.0043 - val_mean_absolute_error: 0.0689\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00280\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0045 - mean_absolute_error: 0.0657 - val_loss: 0.0047 - val_mean_absolute_error: 0.0686\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00280\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0043 - mean_absolute_error: 0.0677 - val_loss: 0.0046 - val_mean_absolute_error: 0.0685\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00280\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0038 - mean_absolute_error: 0.0654 - val_loss: 0.0041 - val_mean_absolute_error: 0.0671\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00280\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0032 - mean_absolute_error: 0.0581 - val_loss: 0.0031 - val_mean_absolute_error: 0.0611\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00280\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0044 - mean_absolute_error: 0.0690 - val_loss: 0.0026 - val_mean_absolute_error: 0.0634\n",
            "\n",
            "Epoch 00196: val_loss improved from 0.00280 to 0.00262, saving model to results/2021-05-07_PLTR-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0037 - mean_absolute_error: 0.0690 - val_loss: 0.0031 - val_mean_absolute_error: 0.0612\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00262\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0030 - mean_absolute_error: 0.0573 - val_loss: 0.0045 - val_mean_absolute_error: 0.0767\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00262\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0044 - mean_absolute_error: 0.0708 - val_loss: 0.0051 - val_mean_absolute_error: 0.0797\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00262\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0031 - mean_absolute_error: 0.0619 - val_loss: 0.0054 - val_mean_absolute_error: 0.0816\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D6UzhYEdoTa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graph(test_df):\n",
        "    \"\"\"\n",
        "    This function plots true close price along with predicted close price\n",
        "    with blue and red colors respectively\n",
        "    \"\"\"\n",
        "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
        "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh7BL9KMd-QN"
      },
      "source": [
        "def get_final_df(model, data):\n",
        "    \"\"\"\n",
        "    This function takes the `model` and `data` dict to \n",
        "    construct a final dataframe that includes the features along \n",
        "    with true and predicted prices of the testing dataset\n",
        "    \"\"\"\n",
        "    # if predicted future price is higher than the current, \n",
        "    # then calculate the true future price minus the current price, to get the buy profit\n",
        "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
        "    # if the predicted future price is lower than the current price,\n",
        "    # then subtract the true future price from the current price\n",
        "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_test = data[\"y_test\"]\n",
        "    # perform prediction and get prices\n",
        "    y_pred = model.predict(X_test)\n",
        "    if SCALE:\n",
        "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    test_df = data[\"test_df\"]\n",
        "    # add predicted future prices to the dataframe\n",
        "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
        "    # add true future prices to the dataframe\n",
        "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
        "    # sort the dataframe by date\n",
        "    test_df.sort_index(inplace=True)\n",
        "    final_df = test_df\n",
        "    # add the buy profit column\n",
        "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    # add the sell profit column\n",
        "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    return final_df"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaUZUCMqezca"
      },
      "source": [
        "def predict(model, data):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    if SCALE:\n",
        "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    else:\n",
        "        predicted_price = prediction[0][0]\n",
        "    return predicted_price"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAAn7VCPe2Xb"
      },
      "source": [
        "# load optimal model weights from results folder\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3dv0dKNe3_k"
      },
      "source": [
        "# evaluate the model\n",
        "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "if SCALE:\n",
        "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "else:\n",
        "    mean_absolute_error = mae"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSwbVLtre6h1"
      },
      "source": [
        "# get the final dataframe for the testing set\n",
        "final_df = get_final_df(model, data)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qezj0l6Ce77p"
      },
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVKg4-N3e9yi"
      },
      "source": [
        "# we calculate the accuracy by counting the number of positive profits\n",
        "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
        "# calculating total buy & sell profit\n",
        "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
        "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
        "# total profit by adding sell & buy together\n",
        "total_profit = total_buy_profit + total_sell_profit\n",
        "# dividing total profit by number of testing samples (number of trades)\n",
        "profit_per_trade = total_profit / len(final_df)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC3YvYhCfI22",
        "outputId": "2ef79ea3-66b2-417c-98c5-85f365a76a44"
      },
      "source": [
        "# printing metrics\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
        "print(f\"{LOSS} loss:\", loss)\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
        "print(\"Accuracy score:\", accuracy_score)\n",
        "print(\"Total buy profit:\", total_buy_profit)\n",
        "print(\"Total sell profit:\", total_sell_profit)\n",
        "print(\"Total profit:\", total_profit)\n",
        "print(\"Profit per trade:\", profit_per_trade)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future price after 15 days is 24.21$\n",
            "huber_loss loss: 0.0026223533786833286\n",
            "Mean Absolute Error: 10.931231869635077\n",
            "Accuracy score: 0.8333333333333334\n",
            "Total buy profit: 33.232662200927734\n",
            "Total sell profit: 16.722898483276367\n",
            "Total profit: 49.9555606842041\n",
            "Profit per trade: 2.775308926900228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "GkfPDjpifp2e",
        "outputId": "7ca5814e-4c62-4500-9deb-4385287c29d1"
      },
      "source": [
        "# plot true/pred prices graph\n",
        "plot_graph(final_df)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}